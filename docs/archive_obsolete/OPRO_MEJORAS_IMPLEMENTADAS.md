# OPRO: Mejoras Implementadas y Pendientes

**Fecha**: 2025-10-13
**Versi√≥n actual**: `run_opro_local_8gb_fixed.py`
**Estado**: Corriendo iteraci√≥n 1 (funcionando bien)

---

## ‚úÖ Mejoras YA Implementadas

### 1. Sanitizaci√≥n de Candidatos
**Archivo**: `run_opro_local_8gb_fixed.py`

```python
def sanitize_prompt(prompt: str) -> Tuple[str, bool]:
    # Bloquea tokens especiales
    forbidden_tokens = ['<|audio_bos|>', '<|AUDIO|>', '<|audio_eos|>', ...]

    # Valida longitud (10-300 chars)
    # Requiere keywords "SPEECH" y "NON-SPEECH"
    # Limpia espacios m√∫ltiples
```

**Resultado**: ‚úì No m√°s crashes por tokens inv√°lidos

### 2. Circuit Breaker
**Archivo**: `run_opro_local_8gb_fixed.py` (l√≠nea ~330)

```python
try:
    ba_clip, ba_cond, metrics = evaluate_prompt(...)
except Exception as e:
    print(f"ERROR evaluating candidate: {e}")
    continue  # Salta al siguiente candidato
```

**Resultado**: ‚úì Una falla no para toda la optimizaci√≥n

### 3. Gesti√≥n de Memoria Mejorada
**Archivo**: `run_opro_local_8gb_fixed.py`

```python
def clear_gpu_memory():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    time.sleep(0.5)  # Da tiempo a GPU para liberar
```

**Resultado**: ‚úì Menos OOM errors en 8GB

### 4. Meta-Prompt Sin Tokens Especiales
**Archivo**: `run_opro_local_8gb_fixed.py` (l√≠nea ~240)

```python
# Limpia prompts en ejemplos
clean_prompt = candidate.prompt.replace('<|audio_bos|><|AUDIO|><|audio_eos|>', '').strip()
```

**Resultado**: ‚úì Optimizador no aprende a generar tokens especiales

### 5. Fallback Inteligente
**Archivo**: `run_opro_local_8gb_fixed.py` (funci√≥n `parse_and_sanitize_candidates`)

```python
if len(candidates_clean) == 0:
    # Fallback: variaciones simples del baseline
    candidates_clean = [
        "Does this audio contain human speech? Answer: SPEECH or NON-SPEECH.",
        "Is there speech in this audio? Reply: SPEECH or NON-SPEECH.",
        ...
    ]
```

**Resultado**: ‚úì Nunca falla por falta de candidatos

---

## üöß Mejoras Pendientes (Prioridad Alta)

### 1. Constrained Decoding en Evaluador
**Status**: ‚è≥ Script creado (`evaluate_prompt_constrained.py`)
**Qu√© falta**: Integrar en el loop de OPRO

**Implementaci√≥n**:
```python
# En run_opro_local_8gb_fixed.py
from evaluate_prompt_constrained import evaluate_prompt_constrained

# Reemplazar evaluate_prompt() con evaluate_prompt_constrained()
ba_clip, ba_cond, ba_hard, metrics = evaluate_prompt_constrained(
    prompt=prompt,
    use_constrained=True,  # Force "SPEECH" o "NONSPEECH"
)
```

**Beneficio**:
- ‚úÖ Salida 100% parseable
- ‚úÖ No m√°s "UNKNOWN" labels
- ‚úÖ Reduce varianza de formato

**Referencia**: [Hugging Face - Constrained Decoding](https://huggingface.co/docs/transformers/main_classes/text_generation)

### 2. Chat Templating Oficial de Qwen2-Audio
**Status**: ‚è≥ Parcialmente implementado
**Qu√© falta**: Verificar que `qwen_audio.py` usa el flujo canon

**Verificaci√≥n necesaria** en `src/qsm/models/qwen_audio.py`:
```python
# ‚úì CORRECTO (debe estar as√≠):
conversation = [
    {"role": "system", "content": self.system_prompt},
    {
        "role": "user",
        "content": [
            {"type": "audio"},  # Audio se pasa separado
            {"type": "text", "text": self.user_prompt},  # Solo texto del usuario
        ],
    },
]

text = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
inputs = self.processor(text=text, audios=audio, sampling_rate=sr, return_tensors="pt")
```

**Referencia**: [Qwen2-Audio Docs](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)

### 3. Reward Enfocado en Condiciones Duras
**Status**: ‚è≥ Script preparado (`evaluate_prompt_constrained.py` tiene `ba_hard`)
**Qu√© falta**: Usar `ba_hard` en reward

**Cambio en reward**:
```python
# ACTUAL (run_opro_local_8gb_fixed.py)
R = BA_clip + 0.25√óBA_cond - 0.05√ólen/100

# PROPUESTO (task-aligned)
R = BA_clip + 0.5√óBA_hard + 0.1√óBA_rest - 0.05√ólen/100
```

**D√≥nde**: `run_opro_local_8gb_fixed.py`, funci√≥n `compute_reward()`

**Beneficio**:
- Empuja mejoras donde el modelo es fr√°gil
- Alineado con objetivos psicof√≠sicos (DT75, SNR-75)

### 4. Successive Halving (Cribado R√°pido)
**Status**: ‚è≥ No implementado
**Complejidad**: Media
**Impacto**: Alto (reduce tiempo 3-5√ó)

**Algoritmo**:
```python
# Genera 8-12 candidatos (en lugar de 3)
candidates = generate_candidates(n=12, temperature=0.8)

# Mini-dev: 20% de muestras (280 en lugar de 1400)
mini_dev_df = split_df.sample(frac=0.2, random_state=seed)

# Eval√∫a TODOS en mini-dev
for prompt in candidates:
    reward_mini = evaluate_prompt(prompt, mini_dev_df)

# Selecciona top-3
top_3 = sorted(candidates, key=lambda c: c.reward)[:3]

# Re-eval√∫a top-3 en dev completo
for prompt in top_3:
    reward_full = evaluate_prompt(prompt, split_df)
```

**Beneficio**:
- Tiempo por iteraci√≥n: ~40 min ‚Üí ~15 min
- Explora m√°s candidatos con el mismo presupuesto

### 5. Deduplicaci√≥n de Candidatos
**Status**: ‚è≥ No implementado
**Complejidad**: Baja
**Impacto**: Medio

**Implementaci√≥n**:
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def deduplicate_candidates(candidates, memory, threshold=0.9):
    """Rechaza candidatos muy similares a memoria."""
    if len(memory) == 0:
        return candidates

    # TF-IDF de candidatos + memoria
    all_prompts = [c.prompt for c in memory] + candidates
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(all_prompts)

    # Similaridad
    mem_vectors = vectors[:len(memory)]
    cand_vectors = vectors[len(memory):]
    similarities = cosine_similarity(cand_vectors, mem_vectors)

    # Filtrar
    unique = []
    for i, prompt in enumerate(candidates):
        if similarities[i].max() < threshold:
            unique.append(prompt)

    return unique
```

**D√≥nde**: Despu√©s de `parse_and_sanitize_candidates()`

### 6. CPU Offloading del Optimizador
**Status**: ‚è≥ No implementado
**Complejidad**: Media
**Impacto**: Alto (en 8GB VRAM)

**Implementaci√≥n**:
```python
# En LocalLLMGenerator.__init__()
from accelerate import cpu_offload

model_kwargs = {
    "device_map": "auto",
    "max_memory": {0: "4GB", "cpu": "16GB"},  # Limita GPU a 4GB
    "offload_folder": "offload_cache",
}
```

**Beneficio**:
- Evaluador residente en GPU (5GB)
- Optimizador offloadeado a CPU/disk
- Evita recargas completas

**Referencia**: [Accelerate - Big Models](https://huggingface.co/docs/accelerate/package_reference/big_modeling)

---

## üìä Mejoras Estad√≠sticas (Post-Optimizaci√≥n)

### 7. Test de McNemar
**Status**: ‚è≥ No implementado
**Cu√°ndo**: Despu√©s de seleccionar best prompt
**D√≥nde**: Nuevo script `scripts/statistical_tests.py`

```python
from statsmodels.stats.contingency_tables import mcnemar

def mcnemar_test(baseline_preds, opro_preds, y_true):
    """Test de McNemar para diferencias pareadas."""
    # Tabla de contingencia
    baseline_correct = (baseline_preds == y_true)
    opro_correct = (opro_preds == y_true)

    # McNemar
    table = pd.crosstab(baseline_correct, opro_correct)
    result = mcnemar(table, exact=True)

    return result.pvalue, result.statistic
```

**Referencia**: [McNemar's Test](https://en.wikipedia.org/wiki/McNemar%27s_test)

### 8. Bootstrap Pareado
**Status**: ‚è≥ No implementado
**Cu√°ndo**: Despu√©s de test set evaluation
**D√≥nde**: `scripts/statistical_tests.py`

```python
def bootstrap_paired_ba(baseline_df, opro_df, n_bootstrap=1000):
    """Bootstrap de Œî(BA_clip) con CI95."""
    deltas = []

    for _ in range(n_bootstrap):
        # Resample clips con reemplazo
        clip_ids = baseline_df["clip_id"].unique()
        sampled_ids = np.random.choice(clip_ids, size=len(clip_ids), replace=True)

        base_sample = baseline_df[baseline_df["clip_id"].isin(sampled_ids)]
        opro_sample = opro_df[opro_df["clip_id"].isin(sampled_ids)]

        ba_base = balanced_accuracy_score(base_sample["y_true"], base_sample["y_pred"])
        ba_opro = balanced_accuracy_score(opro_sample["y_true"], opro_sample["y_pred"])

        deltas.append(ba_opro - ba_base)

    # Percentiles
    ci_lower = np.percentile(deltas, 2.5)
    ci_upper = np.percentile(deltas, 97.5)

    return np.mean(deltas), (ci_lower, ci_upper)
```

---

## üéØ Prioridades para Implementar AHORA

**Mientras corre la optimizaci√≥n actual**:

1. ‚úÖ **Verificar chat templating** en `qwen_audio.py` (5 min)
2. ‚è≥ **Integrar constrained decoding** (15 min)
3. ‚è≥ **Actualizar reward** para incluir `ba_hard` (5 min)

**Despu√©s de que termine (4 horas)**:

4. ‚è≥ **Revisar resultados** y decidir si vale la pena successive halving
5. ‚è≥ **Si resultados buenos**: Transferir al servidor y correr 50 iteraciones
6. ‚è≥ **Si resultados malos**: Implementar successive halving + dedupe y re-correr

**Para el paper**:

7. ‚è≥ **McNemar + bootstrap** en test set
8. ‚è≥ **Pseudo-R¬≤** de curvas psicom√©tricas

---

## üìÅ Archivos Creados

| Archivo | Estado | Prop√≥sito |
|---------|--------|-----------|
| `run_opro_local_8gb.py` | ‚ö†Ô∏è Buggy | Versi√≥n inicial (con crashes) |
| `run_opro_local_8gb_fixed.py` | ‚úÖ Running | Versi√≥n sanitizada (corriendo ahora) |
| `evaluate_prompt_constrained.py` | ‚úÖ Ready | Evaluador con constrained decoding |
| `statistical_tests.py` | ‚è≥ TODO | McNemar + bootstrap |

---

## üîç C√≥mo Monitorear Tu Run Actual

```bash
# Ver mejor prompt actual
cat results/sprint9_opro_laptop_fixed/best_prompt.txt

# Ver progreso
tail -f results/sprint9_opro_laptop_fixed/opro_prompts.jsonl | wc -l
# Divide por 3 para obtener n√∫mero de candidatos evaluados

# Ver GPU
nvidia-smi
```

---

## üìä Resultado Esperado (Tu Run Actual)

**Baseline**: BA_clip = 0.891 (excelente)

**Con sanitizaci√≥n** (tu versi√≥n actual):
- Esperado: BA_clip = 0.893-0.897 (+0.002 a +0.006)
- Con 5 iteraciones: Mejora modesta pero validaci√≥n del pipeline

**Con TODAS las mejoras** (servidor, 50 iteraciones):
- Esperado: BA_clip = 0.900-0.910 (+0.009 a +0.019)
- DT75: 35ms ‚Üí 28-32ms
- SNR-75: -2.9dB ‚Üí -4 a -5dB

---

## ‚úÖ Checklist para Completar Sprint 9

- [x] Implementar sanitizaci√≥n
- [x] Circuit breaker
- [x] Gesti√≥n memoria mejorada
- [x] Meta-prompt limpio
- [x] Evaluador con constrained decoding (script listo)
- [ ] Integrar constrained en OPRO loop
- [ ] Reward con ba_hard
- [ ] Successive halving (opcional, si tiempo)
- [ ] Deduplicaci√≥n (opcional)
- [ ] CPU offloading (opcional)
- [ ] McNemar test (post-optimizaci√≥n)
- [ ] Bootstrap CI (post-optimizaci√≥n)
- [ ] Comparison report
- [ ] Git tag v2.0-opro-baseline

---

**Estado actual**: Tu run est√° funcionando bien con las mejoras cr√≠ticas. Las mejoras adicionales pueden esperar a ver los resultados.
