# Pr√≥ximos Pasos Recomendados

**Fecha**: 2025-10-20
**Estado Actual**: Multi-seed training completado con varianza cero (96.9% en todos los seeds)

---

## Resumen de Resultados Actuales

### Multi-Seed Training (Attention-Only, Re-entrenado)

**Todos los 5 seeds dieron resultados id√©nticos:**

| Seed | Overall | SPEECH | NONSPEECH | Training Loss |
|------|---------|--------|-----------|---------------|
| 42   | 96.9%   | 93.8%  | 100.0%    | 0.2860        |
| 123  | 96.9%   | 93.8%  | 100.0%    | 0.2871        |
| 456  | 96.9%   | 93.8%  | 100.0%    | 0.2858        |
| 789  | 96.9%   | 93.8%  | 100.0%    | 0.2858        |
| 2024 | 96.9%   | 93.8%  | 100.0%    | 0.2839        |

**Estad√≠sticas:**
- Media: 96.9% ¬± 0.0%
- 95% CI: [96.9%, 96.9%]
- Todos fallan en la misma muestra SPEECH (15/16 correctos)
- Todos aciertan todas las muestras NONSPEECH (16/16)

### ¬øPor qu√© varianza cero?

**Causa:** Test set muy peque√±o (32 muestras)
- Solo 16 SPEECH, 16 NONSPEECH
- Solo hay 1-2 "muestras dif√≠ciles" en SPEECH
- Todos los seeds convergen al mismo m√≠nimo local
- 15/16 SPEECH = 93.8% (siempre)

**Esto NO es un bug** - es una consecuencia natural de:
1. Dataset peque√±o (poca granularidad)
2. Arquitectura LoRA muy estable (loss masking funciona excelente)
3. Training muy determinista (mismo proceso ‚Üí mismo resultado)

### Comparaci√≥n: Attention-Only vs MLP (Seed 42)

| Config | Overall | SPEECH | NONSPEECH | Params | Tama√±o |
|--------|---------|--------|-----------|--------|--------|
| **Attention-only (nuevo)** | **96.9%** | 93.8% | **100%** | 20.7M | 84MB |
| MLP (anterior) | 96.9% | **100%** | 93.8% | 43.9M | 168MB |

**Observaci√≥n:**
- **Accuracy total id√©ntica** (96.9%)
- **Trade-off:** Attention-only mejor en NONSPEECH, MLP mejor en SPEECH
- Con solo 32 muestras, imposible distinguir estad√≠sticamente

---

## Plan Recomendado (2 Carriles en Paralelo)

### üéØ Carril A: Evaluaci√≥n Robusta (PRIORITARIO)

**Objetivo:** Blindar la evaluaci√≥n con estad√≠stica s√≥lida

#### Paso 1: Ampliar Test Set (150-200 muestras)

**Por qu√©:**
- Con 32 muestras: 1 error = 3.1% cambio en accuracy
- Con 200 muestras: 1 error = 0.5% cambio en accuracy
- Poder estad√≠stico suficiente para detectar diferencias de 2-3% con p<0.05

**C√≥mo ejecutar:**

```bash
# 1. Generar metadata con dise√±o factorial
python scripts/generate_extended_test_set.py \
    --voxconverse_csv data/raw/voxconverse_metadata.csv \
    --musan_csv data/raw/musan_metadata.csv \
    --output_dir data/processed/extended_test_clips \
    --n_samples_per_class 100 \
    --seed 42

# Esto genera:
# - 200 muestras totales (100 SPEECH, 100 NONSPEECH)
# - Dise√±o factorial: 5 duraciones √ó 5 SNRs √ó 4 samples/condici√≥n
# - Balanceado: 50% SPEECH, 50% NONSPEECH
# - Output: data/processed/extended_test_clips/extended_test_metadata.csv
```

**Nota:** Necesitar√°s implementar `scripts/process_extended_test_clips.py` para procesar el audio seg√∫n la metadata.

#### Paso 2: Re-evaluar con Metodolog√≠a Robusta

```bash
# Evaluar los mejores modelos con el test ampliado
python scripts/run_robust_evaluation.py \
    --models attention_only_seed42 mlp_seed42 baseline \
    --checkpoints \
        checkpoints/qwen2_audio_speech_detection_multiseed/seed_42/final \
        checkpoints/with_mlp_seed_42/final \
        Qwen/Qwen2-Audio-7B-Instruct \
    --test_csv data/processed/extended_test_clips/extended_test_metadata.csv \
    --results_dir results/extended_evaluation
```

#### Paso 3: Decisi√≥n Basada en Estad√≠stica

**Si attention_only ‚âà MLP (p ‚â• 0.05):**
- ‚úÖ **Usar attention-only** (20.7M params, 84MB)
- Raz√≥n: M√°s simple, 2√ó m√°s peque√±o, misma performance
- Continuar con OPRO sobre attention-only

**Si MLP > attention_only (p < 0.05):**
- ‚úÖ **Usar MLP** (43.9M params, 168MB)
- Raz√≥n: Mejora estad√≠sticamente significativa
- Vale la pena el 2√ó en tama√±o por +2-3% accuracy

---

### üöÄ Carril B: OPRO sobre Modelo Fine-Tuned (EN PARALELO)

**Objetivo:** Optimizar el prompt sobre el modelo fine-tuned congelado

**Hip√≥tesis:** Fine-tuning + OPRO > Fine-tuning solo

#### Implementaci√≥n

Necesitar√°s crear `scripts/opro_optimize_finetuned.py` (ver [README_ROBUST_EVALUATION.md](README_ROBUST_EVALUATION.md) para detalles).

**Comando de uso:**

```bash
# Optimizar prompt sobre modelo FT (attention-only seed 42)
python scripts/opro_optimize_finetuned.py \
    --checkpoint checkpoints/qwen2_audio_speech_detection_multiseed/seed_42/final \
    --dev_csv data/processed/normalized_clips/dev_metadata.csv \
    --test_csv data/processed/extended_test_clips/extended_test_metadata.csv \
    --n_iterations 50 \
    --output_dir results/opro_finetuned
```

---

## Resumen: ¬øQu√© Ejecutar Ahora?

### Orden Sugerido (Prioridad Alta ‚Üí Baja)

1. **[URGENTE] Ampliar test set a 200 muestras**
   ```bash
   python scripts/generate_extended_test_set.py \
       --n_samples_per_class 100 \
       --output_dir data/processed/extended_test_clips
   ```
   - Implementar `process_extended_test_clips.py` (basado en `normalize_clips.py`)
   - Tiempo estimado: 2-3 horas desarrollo + 30 min procesamiento

2. **Re-evaluar con estad√≠stica robusta**
   ```bash
   python scripts/run_robust_evaluation.py \
       --models attention_only mlp \
       --test_csv data/processed/extended_test_clips/extended_test_metadata.csv
   ```
   - Tiempo estimado: 15-20 min (evaluaci√≥n con logits es r√°pida)

3. **Decidir modelo final** (attention-only vs MLP)
   - Basado en McNemar p-value
   - Si p ‚â• 0.05 ‚Üí attention-only (m√°s simple)
   - Si p < 0.05 y MLP mejor ‚Üí MLP

4. **[OPCIONAL] OPRO sobre FT**
   - Solo si quieres exprimir √∫ltimos 1-2% de accuracy

5. **[OPCIONAL] Qwen3-Omni baseline**
   - Solo para comparaci√≥n acad√©mica

---

## Timeline Estimado

### Plan Corto (1 d√≠a)
1. ‚úÖ Ampliar test set (2-3 horas)
2. ‚úÖ Re-evaluar attention vs MLP (20 min)
3. ‚úÖ Decidir modelo final (lectura de estad√≠sticas)
4. ‚úÖ Documentar resultados

**Entregable:** Modelo validado estad√≠sticamente con 96-97% accuracy ¬± CI

### Plan Medio (2-3 d√≠as)
1. Todo del plan corto
2. ‚úÖ OPRO sobre modelo FT (6-8 horas training)
3. ‚úÖ Validaci√≥n final en test ampliado
4. ‚úÖ Comparaci√≥n con Qwen3-Omni

**Entregable:** Modelo optimizado con 97-98% accuracy + comparaci√≥n SOTA

### Plan Completo (1 semana)
1. Todo del plan medio
2. ‚úÖ Hyperparameter grid search (LoRA rank, alpha, LR)
3. ‚úÖ Escalado de dataset (train: 500-1k samples)
4. ‚úÖ An√°lisis estratificado (performance por duration, SNR)
5. ‚úÖ Paper-ready results con tablas y estad√≠sticas

**Entregable:** Publicaci√≥n cient√≠fica con resultados s√≥lidos

---

## Archivos de Referencia

**Scripts creados (listos para usar):**
- ‚úÖ `scripts/generate_extended_test_set.py` - Genera metadata de 200 samples
- ‚úÖ `scripts/run_robust_evaluation.py` - Pipeline completo con bootstrap + McNemar
- ‚úÖ `scripts/test_with_logit_scoring.py` - Evaluaci√≥n r√°pida con logits
- ‚úÖ `scripts/compare_models_mcnemar.py` - Test estad√≠stico pareado

**Scripts por implementar:**
- ‚è≥ `scripts/process_extended_test_clips.py` - Procesar audio seg√∫n metadata
- ‚è≥ `scripts/opro_optimize_finetuned.py` - OPRO sobre modelo congelado

**Documentaci√≥n:**
- ‚úÖ `README_ROBUST_EVALUATION.md` - Gu√≠a completa de metodolog√≠a estad√≠stica
- ‚úÖ `NEXT_STEPS.md` - Este documento
- ‚úÖ `RESULTS_MLP_COMPARISON.md` - Resultados MLP vs attention-only

---

**√öltima actualizaci√≥n:** 2025-10-20
**Estado:** Infraestructura lista, esperando decisi√≥n de usuario
