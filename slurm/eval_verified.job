#!/bin/bash
#SBATCH --job-name=eval_ver
#SBATCH --partition=3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=48G
#SBATCH --time=06:00:00
#SBATCH --output=logs/eval_ver_%j.out
#SBATCH --error=logs/eval_ver_%j.err

# Evaluation on VERIFIED dataset

set -euo pipefail

SEED=${1:-42}

echo "============================================"
echo "EVALUATION - VERIFIED DATASET"
echo "============================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Seed: $SEED"
echo "Start time: $(date)"
echo "============================================"

mkdir -p logs

export HF_HOME="/mnt/fast/nobackup/users/$USER/.cache/huggingface"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_HUB_CACHE="$HF_HOME/hub"
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
mkdir -p "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"

source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/.bashrc
conda activate opro || conda activate qwen_audio || { echo "ERROR: Could not activate conda"; exit 1; }
echo "Conda environment: $CONDA_DEFAULT_ENV"

pip install bitsandbytes peft pandas scikit-learn --quiet 2>/dev/null || true

nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv

# Paths for VERIFIED dataset
TEST_CSV="data/processed/expanded_4conditions_verified/test_metadata.csv"
LORA_CHECKPOINT="checkpoints/qwen_lora_verified_seed${SEED}/final"
OPRO_BASE_DIR="results/opro_verified_base_seed${SEED}"
OPRO_LORA_DIR="results/opro_verified_lora_seed${SEED}"
OUTPUT_DIR="results/eval_verified_seed${SEED}"

mkdir -p "$OUTPUT_DIR"

if [ ! -f "$TEST_CSV" ]; then
    echo "ERROR: Test data not found at $TEST_CSV"
    exit 1
fi

# Create prompt files
ORIG_PROMPT="Is this short clip speech or noise? Your answer should be SPEECH or NON-SPEECH."
echo "$ORIG_PROMPT" > "$OUTPUT_DIR/prompt_original.txt"

if [ -f "$OPRO_BASE_DIR/best_prompt.txt" ]; then
    cp "$OPRO_BASE_DIR/best_prompt.txt" "$OUTPUT_DIR/prompt_opro_base.txt"
else
    echo "Assess: Audio contains human speech? A) YES B) NO" > "$OUTPUT_DIR/prompt_opro_base.txt"
    echo "Warning: OPRO-BASE prompt not found, using fallback"
fi

if [ -f "$OPRO_LORA_DIR/best_prompt.txt" ]; then
    cp "$OPRO_LORA_DIR/best_prompt.txt" "$OUTPUT_DIR/prompt_opro_lora.txt"
else
    echo "Assess: Audio contains human speech? A) YES B) NO" > "$OUTPUT_DIR/prompt_opro_lora.txt"
    echo "Warning: OPRO-LORA prompt not found, using fallback"
fi

echo ""
echo "Configuration:"
echo "  Test CSV: $TEST_CSV"
echo "  LoRA: $LORA_CHECKPOINT"
echo "  Output: $OUTPUT_DIR"
echo ""
echo "Prompts:"
echo "  Original: $ORIG_PROMPT"
echo "  OPRO-BASE: $(cat $OUTPUT_DIR/prompt_opro_base.txt)"
echo "  OPRO-LORA: $(cat $OUTPUT_DIR/prompt_opro_lora.txt)"
echo ""

TEST_COUNT=$(wc -l < "$TEST_CSV")
echo "Test samples: $((TEST_COUNT - 1))"
echo ""

# TEST 1: BASE + Original
echo "============================================"
echo "TEST 1: BASE + Original prompt"
echo "============================================"
python scripts/evaluate_with_generation.py \
    --test_csv "$TEST_CSV" \
    --no-lora \
    --prompt_file "$OUTPUT_DIR/prompt_original.txt" \
    --output_csv "$OUTPUT_DIR/eval_base_original.csv"

# TEST 2: BASE + OPRO
echo ""
echo "============================================"
echo "TEST 2: BASE + OPRO prompt"
echo "============================================"
python scripts/evaluate_with_generation.py \
    --test_csv "$TEST_CSV" \
    --no-lora \
    --prompt_file "$OUTPUT_DIR/prompt_opro_base.txt" \
    --output_csv "$OUTPUT_DIR/eval_base_opro.csv"

# TEST 3: LoRA + Original
echo ""
echo "============================================"
echo "TEST 3: LoRA + Original prompt"
echo "============================================"
if [ -d "$LORA_CHECKPOINT" ]; then
    python scripts/evaluate_with_generation.py \
        --test_csv "$TEST_CSV" \
        --checkpoint "$LORA_CHECKPOINT" \
        --prompt_file "$OUTPUT_DIR/prompt_original.txt" \
        --output_csv "$OUTPUT_DIR/eval_lora_original.csv"
else
    echo "WARNING: LoRA checkpoint not found, skipping..."
fi

# TEST 4: LoRA + OPRO
echo ""
echo "============================================"
echo "TEST 4: LoRA + OPRO prompt"
echo "============================================"
if [ -d "$LORA_CHECKPOINT" ]; then
    python scripts/evaluate_with_generation.py \
        --test_csv "$TEST_CSV" \
        --checkpoint "$LORA_CHECKPOINT" \
        --prompt_file "$OUTPUT_DIR/prompt_opro_lora.txt" \
        --output_csv "$OUTPUT_DIR/eval_lora_opro.csv"
else
    echo "WARNING: LoRA checkpoint not found, skipping..."
fi

# Generate results table
echo ""
echo "============================================"
echo "RESULTS SUMMARY"
echo "============================================"

python3 << 'PYTHON_SCRIPT'
import pandas as pd
import json
from pathlib import Path
import sys

output_dir = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("results/eval_verified_seed42")

def calculate_ba(df):
    if 'correct' not in df.columns:
        return None
    speech = df[df['ground_truth'] == 'SPEECH']['correct'].mean() if len(df[df['ground_truth'] == 'SPEECH']) > 0 else 0
    nonspeech = df[df['ground_truth'] == 'NONSPEECH']['correct'].mean() if len(df[df['ground_truth'] == 'NONSPEECH']) > 0 else 0
    return (speech + nonspeech) / 2

results = {}
eval_files = {
    "BASE + Original": "eval_base_original.csv",
    "BASE + OPRO": "eval_base_opro.csv",
    "LoRA + Original": "eval_lora_original.csv",
    "LoRA + OPRO": "eval_lora_opro.csv"
}

for name, filename in eval_files.items():
    filepath = output_dir / filename
    if not filepath.exists():
        continue
    df = pd.read_csv(filepath)
    ba = calculate_ba(df)
    if ba is None:
        continue

    speech_acc = df[df['ground_truth'] == 'SPEECH']['correct'].mean() * 100
    nonspeech_acc = df[df['ground_truth'] == 'NONSPEECH']['correct'].mean() * 100

    results[name] = {
        "BA": round(ba * 100, 2),
        "Speech": round(speech_acc, 2),
        "Nonspeech": round(nonspeech_acc, 2)
    }

print("\n" + "=" * 60)
print("RESULTS - VERIFIED DATASET")
print("=" * 60)
print(f"{'Configuration':<20} {'BA':>10} {'Speech':>10} {'Nonspeech':>10}")
print("-" * 60)
for name, metrics in results.items():
    print(f"{name:<20} {metrics['BA']:>10.2f} {metrics['Speech']:>10.2f} {metrics['Nonspeech']:>10.2f}")
print("=" * 60)

with open(output_dir / "results_summary.json", "w") as f:
    json.dump(results, f, indent=2)
print(f"\nResults saved to: {output_dir / 'results_summary.json'}")
PYTHON_SCRIPT

echo ""
echo "============================================"
echo "EVALUATION COMPLETE"
echo "============================================"
echo "End time: $(date)"
