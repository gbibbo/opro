#!/bin/bash
#SBATCH --job-name=train_lora_full
#SBATCH --partition=3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=48G
#SBATCH --time=08:00:00
#SBATCH --output=logs/train_lora_full_%j.out
#SBATCH --error=logs/train_lora_full_%j.err

# ==============================================================================
# Train LoRA with FULL Dataset (train + dev combined)
# ==============================================================================
#
# Trains LoRA adapter using all available training data for better generalization.
# Uses train + dev as training set, test as validation.
#
# Usage:
#   sbatch slurm/train_lora_full_data.job [seed]
#
# ==============================================================================

set -e
set -u

SEED=${1:-42}

echo "============================================"
echo "TRAIN LORA WITH FULL DATA"
echo "============================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Seed: $SEED"
echo "Start time: $(date)"
echo "============================================"

mkdir -p logs

# Setup environment
export HF_HOME="/mnt/fast/nobackup/users/$USER/.cache/huggingface"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_HUB_CACHE="$HF_HOME/hub"
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
mkdir -p "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"

# Activate conda
source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/.bashrc
conda activate opro || conda activate qwen_audio || { echo "ERROR: Could not activate conda"; exit 1; }
echo "Conda environment: $CONDA_DEFAULT_ENV"

# Ensure dependencies
pip install bitsandbytes peft --quiet 2>/dev/null || true

# GPU info
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv

# Check available data
echo ""
echo "Checking available data..."
ls -la data/processed/base_1000ms/*.csv 2>/dev/null || true

# Combine train + dev for training (more data)
echo ""
echo "Preparing combined training manifest..."
python - <<'PY'
import pandas as pd
from pathlib import Path

base_dir = Path("data/processed/base_1000ms")

# Load all splits
train_df = pd.read_csv(base_dir / "train_base.csv") if (base_dir / "train_base.csv").exists() else pd.DataFrame()
dev_df = pd.read_csv(base_dir / "dev_base.csv") if (base_dir / "dev_base.csv").exists() else pd.DataFrame()
test_df = pd.read_csv(base_dir / "test_base.csv") if (base_dir / "test_base.csv").exists() else pd.DataFrame()

print(f"Train samples: {len(train_df)}")
print(f"Dev samples: {len(dev_df)}")
print(f"Test samples: {len(test_df)}")

# Combine train + dev for larger training set
if len(train_df) > 0 and len(dev_df) > 0:
    combined_train = pd.concat([train_df, dev_df], ignore_index=True)
else:
    combined_train = train_df if len(train_df) > 0 else dev_df

# Ensure we have required columns
if "ground_truth" in combined_train.columns and "label" not in combined_train.columns:
    combined_train["label"] = combined_train["ground_truth"]
if "ground_truth" in test_df.columns and "label" not in test_df.columns:
    test_df["label"] = test_df["ground_truth"]

# Save combined manifest
output_dir = Path("data/processed/training_full")
output_dir.mkdir(parents=True, exist_ok=True)

combined_train.to_csv(output_dir / "train_combined.csv", index=False)
test_df.to_csv(output_dir / "test.csv", index=False)

print(f"\nCombined train manifest: {len(combined_train)} samples")
print(f"Test manifest: {len(test_df)} samples")
print(f"Label distribution in train:")
print(combined_train["ground_truth"].value_counts().to_string())
print(f"\nSaved to: {output_dir}")
PY

# Set output directory
OUTPUT_DIR="checkpoints/qwen_lora_full_seed${SEED}"

echo ""
echo "Configuration:"
echo "  Train CSV: data/processed/training_full/train_combined.csv"
echo "  Val CSV: data/processed/training_full/test.csv"
echo "  Output: $OUTPUT_DIR"
echo "  Seed: $SEED"
echo ""

# Run training
python scripts/finetune_qwen_audio.py \
    --seed $SEED \
    --output_dir "$OUTPUT_DIR" \
    --train_csv data/processed/training_full/train_combined.csv \
    --val_csv data/processed/training_full/test.csv \
    --num_epochs 3 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4 \
    --lora_r 16 \
    --lora_alpha 32

EXIT_CODE=$?

echo ""
echo "============================================"
echo "TRAINING COMPLETE"
echo "============================================"
echo "Exit code: $EXIT_CODE"
echo "End time: $(date)"

if [ $EXIT_CODE -eq 0 ]; then
    echo ""
    echo "Model saved to: $OUTPUT_DIR/final"
    ls -la "$OUTPUT_DIR/final/" 2>/dev/null || true

    # Quick evaluation
    echo ""
    echo "Running quick evaluation..."
    python - "$OUTPUT_DIR/final" <<'PY'
import sys
from pathlib import Path
import pandas as pd
import torch

checkpoint = sys.argv[1]
sys.path.insert(0, str(Path.cwd()))

print(f"\nEvaluating checkpoint: {checkpoint}")

# Load model
from src.qsm.models.qwen_audio import Qwen2AudioClassifier
from peft import PeftModel

model = Qwen2AudioClassifier(
    model_name="Qwen/Qwen2-Audio-7B-Instruct",
    device="cuda",
    load_in_4bit=True,
)
model.model = PeftModel.from_pretrained(
    model.model, checkpoint, torch_dtype=torch.float16
)
model.model.eval()

# Quick test on a few samples
test_df = pd.read_csv("data/processed/training_full/test.csv")
print(f"Testing on {min(20, len(test_df))} samples...")

correct = 0
for i, row in test_df.head(20).iterrows():
    audio_path = row["audio_path"]
    if not audio_path.startswith("data/"):
        audio_path = "data/" + audio_path

    try:
        result = model.predict(audio_path, return_scores=True)
        p_first = result.probs.get("p_first_token", 0.5) if result.probs else 0.5
        pred = "SPEECH" if p_first > 0.5 else "NONSPEECH"
        gt = row["ground_truth"]
        if pred == gt:
            correct += 1
    except:
        pass

print(f"Quick test accuracy: {correct}/20 = {correct/20*100:.1f}%")
PY
fi

exit $EXIT_CODE
