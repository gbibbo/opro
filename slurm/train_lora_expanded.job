#!/bin/bash
#SBATCH --job-name=train_expanded
#SBATCH --partition=3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=48G
#SBATCH --time=12:00:00
#SBATCH --output=logs/train_expanded_%j.out
#SBATCH --error=logs/train_expanded_%j.err

# ==============================================================================
# Train LoRA with EXPANDED Dataset (~20,736 samples)
# ==============================================================================
#
# Uses the expanded dataset with 6x more data than the original:
# - ~14,515 train samples (vs 3,072 original)
# - ~4,147 dev samples (vs 3,456 original)
# - Balanced SPEECH/NONSPEECH
# - Full augmentation: 8 durations Ã— 6 SNRs
#
# Parameters matched to the original successful training:
# - Learning rate: 5e-5 (not 2e-4!)
# - Batch size: 1
# - 3 epochs
#
# ==============================================================================

set -e
set -u

SEED=${1:-42}

echo "============================================"
echo "TRAIN LORA WITH EXPANDED DATASET"
echo "============================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Seed: $SEED"
echo "Start time: $(date)"
echo "============================================"

mkdir -p logs

# Setup environment
export HF_HOME="/mnt/fast/nobackup/users/$USER/.cache/huggingface"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_HUB_CACHE="$HF_HOME/hub"
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
mkdir -p "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"

# Activate conda
source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/.bashrc
conda activate opro || conda activate qwen_audio || { echo "ERROR: Could not activate conda"; exit 1; }
echo "Conda environment: $CONDA_DEFAULT_ENV"

# Ensure dependencies
pip install bitsandbytes peft --quiet 2>/dev/null || true

# GPU info
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv

# Check data exists
TRAIN_CSV="data/processed/expanded_dataset/train_metadata.csv"
DEV_CSV="data/processed/expanded_dataset/dev_metadata.csv"

if [ ! -f "$TRAIN_CSV" ]; then
    echo "ERROR: Training data not found at $TRAIN_CSV"
    echo "Run generate_expanded_dataset.job first!"
    exit 1
fi

echo ""
echo "Dataset info:"
echo "  Train: $(wc -l < $TRAIN_CSV) samples"
echo "  Dev: $(wc -l < $DEV_CSV) samples"

# Set output directory
OUTPUT_DIR="checkpoints/qwen_lora_expanded_seed${SEED}"

echo ""
echo "Configuration:"
echo "  Train CSV: $TRAIN_CSV"
echo "  Val CSV: $DEV_CSV"
echo "  Output: $OUTPUT_DIR"
echo "  Seed: $SEED"
echo "  Learning rate: 5e-5 (original successful rate)"
echo "  Batch size: 1"
echo "  Epochs: 3"
echo ""

# Run training with ORIGINAL successful parameters
python scripts/finetune_qwen_audio.py \
    --seed $SEED \
    --output_dir "$OUTPUT_DIR" \
    --train_csv "$TRAIN_CSV" \
    --val_csv "$DEV_CSV" \
    --num_epochs 3 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --learning_rate 5e-5 \
    --lora_r 16 \
    --lora_alpha 32

EXIT_CODE=$?

echo ""
echo "============================================"
echo "TRAINING COMPLETE"
echo "============================================"
echo "Exit code: $EXIT_CODE"
echo "End time: $(date)"

if [ $EXIT_CODE -eq 0 ]; then
    echo ""
    echo "Model saved to: $OUTPUT_DIR/final"
    ls -la "$OUTPUT_DIR/final/" 2>/dev/null || true
fi

exit $EXIT_CODE
