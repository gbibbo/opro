#!/bin/bash
#SBATCH --job-name=eval_test
#SBATCH --partition=3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=48G
#SBATCH --time=04:00:00
#SBATCH --output=logs/eval_test_%j.out
#SBATCH --error=logs/eval_test_%j.err

# ==============================================================================
# Evaluate best OPRO prompt on TEST set (single evaluation, no optimization)
# ==============================================================================

set -e
set -u

echo "============================================"
echo "EVALUATE BEST PROMPT ON TEST"
echo "============================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "Start time: $(date)"
echo "============================================"

mkdir -p logs

# Setup environment
export HF_HOME="/mnt/fast/nobackup/users/$USER/.cache/huggingface"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_HUB_CACHE="$HF_HOME/hub"
mkdir -p "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"

# Activate conda
source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/.bashrc
conda activate opro || conda activate qwen_audio || { echo "ERROR: Could not activate conda"; exit 1; }
echo "Conda environment: $CONDA_DEFAULT_ENV"

# Ensure bitsandbytes
if ! python -c "import bitsandbytes" 2>/dev/null; then
    pip install bitsandbytes --no-cache-dir
fi

# GPU info
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv

# Symlink for path compatibility
ln -sfn "$(pwd)/data/processed" processed

# Best prompt from OPRO optimization
BEST_PROMPT="Is this short clip speech or noise? Your answer should be SPEECH or NON-SPEECH."

# Output directory
OUTPUT_DIR="results/opro_test_final"
mkdir -p "$OUTPUT_DIR"

echo ""
echo "Configuration:"
echo "  Test manifest: data/processed/experimental_variants/test_metadata.csv"
echo "  Best prompt: $BEST_PROMPT"
echo "  Output: $OUTPUT_DIR"
echo ""

# Run evaluation on TEST
python - <<'PY'
import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm

# Add src to path
sys.path.insert(0, str(Path.cwd()))
from src.qsm.models.qwen_audio import Qwen2AudioClassifier
from src.qsm.utils.normalize import normalize_to_binary

# Configuration
TEST_MANIFEST = "data/processed/experimental_variants/test_metadata.csv"
BEST_PROMPT = "Is this short clip speech or noise? Your answer should be SPEECH or NON-SPEECH."
OUTPUT_DIR = Path("results/opro_test_final")

print(f"\n{'='*60}")
print("EVALUATING BEST PROMPT ON TEST SET")
print(f"{'='*60}")
print(f"Prompt: {BEST_PROMPT}")
print(f"Manifest: {TEST_MANIFEST}")

# Load test data
df = pd.read_csv(TEST_MANIFEST)
if "ground_truth" in df.columns and "label" not in df.columns:
    df["label"] = df["ground_truth"]

print(f"Test samples: {len(df)}")

# Resolve paths
def resolve_path(p):
    p_str = str(p).replace("\\", "/")
    if os.path.isabs(p_str) and os.path.isfile(p_str):
        return p_str
    if os.path.isfile(p_str):
        return os.path.abspath(p_str)
    if p_str.startswith("processed/"):
        candidate = os.path.join("data", p_str)
        if os.path.isfile(candidate):
            return os.path.abspath(candidate)
    candidate = os.path.join(os.getcwd(), p_str)
    if os.path.isfile(candidate):
        return candidate
    return p_str

df["audio_resolved"] = df["audio_path"].map(resolve_path)
exist_mask = df["audio_resolved"].map(lambda x: Path(x).is_file())
print(f"Files found: {exist_mask.mean():.1%} ({exist_mask.sum()}/{len(df)})")

if exist_mask.mean() < 0.95:
    print("ERROR: Too many missing files!")
    sys.exit(1)

df = df[exist_mask].copy()
print(f"Proceeding with {len(df)} samples")

# Load model
print("\nLoading Qwen2-Audio model...")
model = Qwen2AudioClassifier(
    model_name="Qwen/Qwen2-Audio-7B-Instruct",
    device="cuda",
    load_in_4bit=True,
)
model.user_prompt = BEST_PROMPT

# Evaluate
results = []
for _, row in tqdm(df.iterrows(), total=len(df), desc="Evaluating"):
    audio_path = row["audio_resolved"]
    ground_truth = row["label"]

    try:
        result = model.predict(audio_path, return_scores=True)
        normalized_label, confidence = normalize_to_binary(
            result.raw_output,
            probs=result.probs,
            mode="auto",
            verbalizers=["SPEECH", "NONSPEECH"],
        )
        if normalized_label is None:
            normalized_label = result.label

        is_correct = (normalized_label == ground_truth) if normalized_label else False

        # Derive condition
        dm = row.get("duration_ms", None)
        sn = row.get("snr_db", None)
        if pd.notna(dm) and pd.notna(sn):
            condition = f"dur_{int(dm)}_snr_{int(sn)}"
        else:
            condition = "unknown"

        results.append({
            "audio_path": str(audio_path),
            "ground_truth": ground_truth,
            "prediction": normalized_label,
            "correct": is_correct,
            "condition": condition,
            "duration_ms": dm,
            "snr_db": sn,
        })
    except Exception as e:
        print(f"Error: {audio_path}: {e}")

# Calculate metrics
results_df = pd.DataFrame(results)
results_df.to_csv(OUTPUT_DIR / "test_predictions.csv", index=False)

# Overall metrics
speech = results_df[results_df["ground_truth"] == "SPEECH"]
nonspeech = results_df[results_df["ground_truth"] == "NONSPEECH"]

speech_acc = speech["correct"].mean() if len(speech) > 0 else 0
nonspeech_acc = nonspeech["correct"].mean() if len(nonspeech) > 0 else 0
ba_clip = (speech_acc + nonspeech_acc) / 2

# Per-condition metrics
condition_metrics = {}
for cond in results_df["condition"].unique():
    cond_df = results_df[results_df["condition"] == cond]
    cond_speech = cond_df[cond_df["ground_truth"] == "SPEECH"]
    cond_nonspeech = cond_df[cond_df["ground_truth"] == "NONSPEECH"]
    cond_speech_acc = cond_speech["correct"].mean() if len(cond_speech) > 0 else 0
    cond_nonspeech_acc = cond_nonspeech["correct"].mean() if len(cond_nonspeech) > 0 else 0
    cond_ba = (cond_speech_acc + cond_nonspeech_acc) / 2
    condition_metrics[cond] = {
        "ba": float(cond_ba),
        "speech_acc": float(cond_speech_acc),
        "nonspeech_acc": float(cond_nonspeech_acc),
        "n_samples": int(len(cond_df)),
    }

ba_conditions = np.mean([m["ba"] for m in condition_metrics.values()])

# Per duration
duration_metrics = {}
for dur in results_df["duration_ms"].dropna().unique():
    dur_df = results_df[results_df["duration_ms"] == dur]
    dur_speech = dur_df[dur_df["ground_truth"] == "SPEECH"]
    dur_nonspeech = dur_df[dur_df["ground_truth"] == "NONSPEECH"]
    dur_speech_acc = dur_speech["correct"].mean() if len(dur_speech) > 0 else 0
    dur_nonspeech_acc = dur_nonspeech["correct"].mean() if len(dur_nonspeech) > 0 else 0
    dur_ba = (dur_speech_acc + dur_nonspeech_acc) / 2
    duration_metrics[int(dur)] = {"ba": float(dur_ba), "n_samples": int(len(dur_df))}

# Per SNR
snr_metrics = {}
for snr in results_df["snr_db"].dropna().unique():
    snr_df = results_df[results_df["snr_db"] == snr]
    snr_speech = snr_df[snr_df["ground_truth"] == "SPEECH"]
    snr_nonspeech = snr_df[snr_df["ground_truth"] == "NONSPEECH"]
    snr_speech_acc = snr_speech["correct"].mean() if len(snr_speech) > 0 else 0
    snr_nonspeech_acc = snr_nonspeech["correct"].mean() if len(snr_nonspeech) > 0 else 0
    snr_ba = (snr_speech_acc + snr_nonspeech_acc) / 2
    snr_metrics[int(snr)] = {"ba": float(snr_ba), "n_samples": int(len(snr_df))}

# Save final metrics
final_metrics = {
    "prompt": BEST_PROMPT,
    "split": "test",
    "n_samples": len(results_df),
    "ba_clip": float(ba_clip),
    "ba_conditions": float(ba_conditions),
    "speech_acc": float(speech_acc),
    "nonspeech_acc": float(nonspeech_acc),
    "duration_metrics": duration_metrics,
    "snr_metrics": snr_metrics,
    "condition_metrics": condition_metrics,
}

with open(OUTPUT_DIR / "test_final_metrics.json", "w") as f:
    json.dump(final_metrics, f, indent=2)

# Print summary
print(f"\n{'='*60}")
print("TEST SET RESULTS")
print(f"{'='*60}")
print(f"Prompt: {BEST_PROMPT}")
print(f"\nOverall:")
print(f"  BA_clip:       {ba_clip:.4f} ({ba_clip*100:.2f}%)")
print(f"  BA_conditions: {ba_conditions:.4f} ({ba_conditions*100:.2f}%)")
print(f"  Speech acc:    {speech_acc:.4f} ({speech_acc*100:.2f}%)")
print(f"  Nonspeech acc: {nonspeech_acc:.4f} ({nonspeech_acc*100:.2f}%)")
print(f"\nBy duration (ms):")
for dur in sorted(duration_metrics.keys()):
    m = duration_metrics[dur]
    print(f"  {dur:4d}ms: BA={m['ba']:.3f} (n={m['n_samples']})")
print(f"\nBy SNR (dB):")
for snr in sorted(snr_metrics.keys()):
    m = snr_metrics[snr]
    print(f"  {snr:3d}dB: BA={m['ba']:.3f} (n={m['n_samples']})")

print(f"\nResults saved to: {OUTPUT_DIR}")
print(f"{'='*60}")
PY

EXIT_CODE=$?

echo ""
echo "============================================"
echo "TEST EVALUATION COMPLETE"
echo "============================================"
echo "Exit code: $EXIT_CODE"
echo "End time: $(date)"

if [ $EXIT_CODE -eq 0 ]; then
    echo ""
    echo "Results:"
    cat results/opro_test_final/test_final_metrics.json | head -20
fi

exit $EXIT_CODE
