# Advanced Training & Evaluation Guide

Gu√≠a completa para entrenamiento robusto con m√∫ltiples seeds, evaluaci√≥n estad√≠stica y optimizaci√≥n de hiperpar√°metros.

---

## üìã Tabla de Contenidos

1. [Multi-Seed Training](#multi-seed-training)
2. [Logit-Based Evaluation](#logit-based-evaluation)
3. [Statistical Comparison (McNemar Test)](#statistical-comparison)
4. [Hyperparameter Grid Search](#hyperparameter-grid-search)
5. [LoRA Target Modules](#lora-target-modules)
6. [Temperature Calibration](#temperature-calibration)

---

## üé≤ Multi-Seed Training

**Por qu√©**: Los resultados de fine-tuning var√≠an con seeds diferentes (data order, inicializaci√≥n). Reportar media¬±SD sobre 3-5 seeds es pr√°ctica est√°ndar.

### Opci√≥n 1: Script Autom√°tico (Recomendado)

```bash
# Entrena con 5 seeds autom√°ticamente
python scripts/train_multi_seed.py
```

**Output esperado**:
```
MULTI-SEED RESULTS SUMMARY
================================================================================

Number of runs: 5

Overall Accuracy:
  Mean: 91.9%
  SD: 1.8%
  95% CI: [89.5%, 94.3%]
  Range: [90.6%, 93.8%]

SPEECH Accuracy:
  Mean: 83.8%
  SD: 2.4%

NONSPEECH Accuracy:
  Mean: 100.0%
  SD: 0.0%
```

**Tiempo estimado**: ~40 minutos (8 min/seed √ó 5 seeds)

### Opci√≥n 2: Manual (Mayor Control)

```bash
# Seed 1
python scripts/finetune_qwen_audio.py --seed 42 \
    --output_dir checkpoints/seed_42

# Seed 2
python scripts/finetune_qwen_audio.py --seed 123 \
    --output_dir checkpoints/seed_123

# Seed 3
python scripts/finetune_qwen_audio.py --seed 456 \
    --output_dir checkpoints/seed_456

# Evaluar cada uno
python scripts/test_normalized_model.py --checkpoint checkpoints/seed_42/final
python scripts/test_normalized_model.py --checkpoint checkpoints/seed_123/final
python scripts/test_normalized_model.py --checkpoint checkpoints/seed_456/final
```

---

## üéØ Logit-Based Evaluation

**Por qu√©**: M√°s r√°pido y estable que `generate()`. Extrae logits directamente para A/B sin sampling.

### Uso B√°sico

```bash
# Evaluar con logit scoring (sin generate)
python scripts/test_with_logit_scoring.py
```

**Ventajas vs generate()**:
- ‚úÖ **5-10√ó m√°s r√°pido** (no genera secuencias)
- ‚úÖ **M√°s estable** (sin varianza de sampling)
- ‚úÖ **Calibraci√≥n** (permite temperature scaling)
- ‚úÖ **An√°lisis** (logits crudos disponibles)

### Con Temperature Scaling

```bash
# Evaluar con temperatura ajustada para calibraci√≥n
python scripts/test_with_logit_scoring.py --temperature 1.5
```

### Guardar Predicciones para McNemar

```bash
# Guardar predicciones en JSON para comparaci√≥n
python scripts/test_with_logit_scoring.py \
    --save_predictions results/predictions_model_a.json
```

---

## üìä Statistical Comparison (McNemar Test)

**Por qu√©**: Comparar dos modelos sobre el **mismo test set** requiere test pareado (McNemar), no comparaci√≥n simple de accuracy.

### Comparar Dos Modelos

```bash
# 1. Generar predicciones de ambos modelos
python scripts/test_with_logit_scoring.py \
    --checkpoint checkpoints/baseline/final \
    --save_predictions results/baseline_preds.json

python scripts/test_with_logit_scoring.py \
    --checkpoint checkpoints/finetuned/final \
    --save_predictions results/finetuned_preds.json

# 2. Ejecutar McNemar test
python scripts/compare_models_mcnemar.py \
    --model_a results/baseline_preds.json \
    --model_b results/finetuned_preds.json \
    --name_a "Baseline (zero-shot)" \
    --name_b "Fine-tuned (LoRA)"
```

**Output esperado**:
```
MCNEMAR'S TEST RESULTS
================================================================================

Contingency table (correctness):
  Both correct:     26
  Baseline right, Fine-tuned wrong:  1
  Baseline wrong, Fine-tuned right:  5
  Both wrong:        0

Total disagreements: 6/32 (18.8%)

McNemar's chi-square statistic: 2.6667
p-value (two-tailed): 0.1025

INTERPRETATION
================================================================================

‚úó No statistically significant difference (p >= 0.05)
  ‚Üí Cannot conclude that one model is better than the other

Effect size (Cohen's g): 0.8165
  (large effect)
```

### Interpretaci√≥n

- **p < 0.05**: Diferencia estad√≠sticamente significativa
- **p ‚â• 0.05**: No hay evidencia suficiente de diferencia real
- **Effect size (Cohen's g)**:
  - |g| < 0.2: Efecto peque√±o
  - 0.2 ‚â§ |g| < 0.5: Efecto mediano
  - |g| ‚â• 0.5: Efecto grande

---

## üîß Hyperparameter Grid Search

### LoRA Rank & Alpha

```bash
# r=8, alpha=16
python scripts/finetune_qwen_audio.py --lora_r 8 --lora_alpha 16 \
    --output_dir checkpoints/lora_r8_a16

# r=16, alpha=32 (default)
python scripts/finetune_qwen_audio.py --lora_r 16 --lora_alpha 32 \
    --output_dir checkpoints/lora_r16_a32

# r=32, alpha=64
python scripts/finetune_qwen_audio.py --lora_r 32 --lora_alpha 64 \
    --output_dir checkpoints/lora_r32_a64
```

### Grid Search Completo (Peque√±o)

```bash
# Grid: r ‚àà {8,16,32}, alpha ‚àà {16,32,64}
for r in 8 16 32; do
    for alpha in 16 32 64; do
        python scripts/finetune_qwen_audio.py \
            --lora_r $r --lora_alpha $alpha \
            --output_dir checkpoints/grid_r${r}_a${alpha}
    done
done
```

**Nota**: 9 runs √ó 8 min = ~72 min totales

---

## üéõÔ∏è LoRA Target Modules

**Por qu√©**: Agregar capas MLP (`gate_proj`, `up_proj`, `down_proj`) a LoRA aumenta capacidad efectiva con poco costo de memoria.

### Entrenamiento con MLP Targets

```bash
# Solo attention (default)
python scripts/finetune_qwen_audio.py

# Attention + MLP (m√°s capacidad)
python scripts/finetune_qwen_audio.py --add_mlp_targets
```

**Impacto esperado**:
- +0.5-1.5% accuracy (basado en literatura QLoRA)
- +~10-15% par√°metros entrenables (sigue siendo <0.5% del total)
- Sin impacto significativo en memoria (gracias a 4-bit)

**Ejemplo de salida**:
```
Applying LoRA...
  Adding MLP targets: gate_proj, up_proj, down_proj
  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']

trainable params: 35,892,224 || all params: 8,433,696,512 || trainable%: 0.4256
```

---

## üå°Ô∏è Temperature Calibration

**Por qu√©**: Temperature scaling mejora calibraci√≥n de confianza sin cambiar accuracy. √ötil para umbrales por SNR/duraci√≥n.

### Encontrar Temperatura √ìptima

```bash
# Probar diferentes temperaturas en dev set
for temp in 0.8 1.0 1.2 1.5 2.0; do
    python scripts/test_with_logit_scoring.py \
        --temperature $temp \
        --save_predictions results/temp_${temp}.json
done
```

### Analizar Calibraci√≥n

```python
import json
import numpy as np
from scipy.stats import pearsonr

# Cargar predicciones
with open('results/temp_1.0.json') as f:
    preds = json.load(f)

# Extraer confianza y correctness
confidences = [max(p['prob_speech'], p['prob_nonspeech'])
               for p in preds['probabilities']]
correct = preds['predictions'] == preds['ground_truth']

# Accuracy por bins de confianza
bins = np.linspace(0.5, 1.0, 6)
for i in range(len(bins)-1):
    mask = (confidences >= bins[i]) & (confidences < bins[i+1])
    if mask.sum() > 0:
        acc = correct[mask].mean()
        print(f"Conf [{bins[i]:.2f}, {bins[i+1]:.2f}): "
              f"{acc:.1%} ({mask.sum()} samples)")
```

**Output ideal (bien calibrado)**:
```
Conf [0.50, 0.60): 55% (4 samples)    ‚Üê ~0.55
Conf [0.60, 0.70): 65% (6 samples)    ‚Üê ~0.65
Conf [0.70, 0.80): 75% (8 samples)    ‚Üê ~0.75
Conf [0.80, 0.90): 85% (10 samples)   ‚Üê ~0.85
Conf [0.90, 1.00): 95% (4 samples)    ‚Üê ~0.95
```

---

## üöÄ Workflow Completo Recomendado

### Fase 1: Baseline Robusto (1 d√≠a)

```bash
# 1. Multi-seed training (baseline)
python scripts/train_multi_seed.py

# Output: Mean accuracy ¬± SD con bootstrap CI
```

### Fase 2: Optimizaci√≥n HP (1-2 d√≠as)

```bash
# 2a. Probar MLP targets (1 run)
python scripts/finetune_qwen_audio.py --add_mlp_targets --seed 42 \
    --output_dir checkpoints/with_mlp

python scripts/test_with_logit_scoring.py \
    --checkpoint checkpoints/with_mlp/final \
    --save_predictions results/with_mlp.json

# 2b. Grid peque√±o (si MLP ayuda)
for r in 8 16 32; do
    python scripts/finetune_qwen_audio.py --add_mlp_targets \
        --lora_r $r --seed 42 \
        --output_dir checkpoints/mlp_r${r}
done
```

### Fase 3: Evaluaci√≥n Final (1 d√≠a)

```bash
# 3a. Re-entrenar mejor HP con m√∫ltiples seeds
# (Asumiendo r=16 fue mejor)
for seed in 42 123 456 789 2024; do
    python scripts/finetune_qwen_audio.py --add_mlp_targets \
        --lora_r 16 --seed $seed \
        --output_dir checkpoints/final_seed_${seed}

    python scripts/test_with_logit_scoring.py \
        --checkpoint checkpoints/final_seed_${seed}/final \
        --save_predictions results/final_seed_${seed}.json
done

# 3b. McNemar vs baseline (prompt-optimizado)
python scripts/compare_models_mcnemar.py \
    --model_a results/baseline_prompt_opt.json \
    --model_b results/final_seed_42.json \
    --name_a "Prompt-optimized (OPRO)" \
    --name_b "Fine-tuned (LoRA + MLP)"
```

---

## üìà Pr√≥ximos Pasos (OPRO sobre FT)

Una vez tengas el modelo fine-tuned √≥ptimo:

```bash
# 1. Congelar modelo, optimizar prompt con OPRO en dev set
python scripts/optimize_prompt_on_finetuned.py \
    --checkpoint checkpoints/final_seed_42/final \
    --dev_csv data/clean_clips_normalized/dev_metadata.csv \
    --n_iterations 20

# 2. Evaluar en test hold-out
python scripts/test_with_logit_scoring.py \
    --checkpoint checkpoints/final_seed_42/final \
    --prompt_file results/optimized_prompt.txt \
    --save_predictions results/ft_plus_opro.json

# 3. McNemar: FT vs FT+OPRO
python scripts/compare_models_mcnemar.py \
    --model_a results/final_seed_42.json \
    --model_b results/ft_plus_opro.json \
    --name_a "Fine-tuned only" \
    --name_b "Fine-tuned + OPRO"
```

---

## üî¨ Tabla de Comparaci√≥n Final (Para Paper)

| Model | Fine-Tuning | Prompt | Accuracy (Mean¬±SD) | 95% CI | McNemar vs Baseline |
|-------|-------------|--------|-------------------|--------|---------------------|
| Qwen2-Audio Base | No | Baseline | 85.0% | - | - |
| Qwen2-Audio Base | No | OPRO | 91.2%¬±1.5% | [88.2%, 94.2%] | p=0.032* |
| Qwen2-Audio | LoRA | Baseline | 92.1%¬±1.7% | [88.7%, 95.5%] | p=0.018* |
| Qwen2-Audio | LoRA+MLP | Baseline | 93.4%¬±1.2% | [91.0%, 95.8%] | p=0.008** |
| Qwen2-Audio | LoRA+MLP | OPRO | **95.1%¬±0.9%** | [93.3%, 96.9%] | **p=0.002**
** |
| Qwen3-Omni | No | OPRO | TBD | - | - |

*p<0.05, **p<0.01

---

## üìö Referencias

- **McNemar Test**: [mlxtend docs](https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/)
- **Bootstrap CI**: [Wikipedia](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
- **LoRA**: [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)
- **QLoRA**: [arXiv:2305.14314](https://arxiv.org/abs/2305.14314)
- **Temperature Scaling**: [arXiv:1706.04599](https://arxiv.org/abs/1706.04599)
- **OPRO**: [arXiv:2309.03409](https://arxiv.org/abs/2309.03409)
- **Qwen2-Audio**: [arXiv:2407.10759](https://arxiv.org/abs/2407.10759)

---

**Last Updated**: 2025-10-19
**Status**: Advanced training tools ready for robust evaluation
