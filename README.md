# Qwen Speech Minimum (QSM)

**Status:** âœ… Sprint 3 Complete | ðŸš€ VAD Baseline Established

Temporal threshold measurement and optimization for speech detection in Qwen models.

## ðŸ“Š Project Status

- **Sprint 0 (Infrastructure):** âœ… COMPLETE
- **Sprint 1 (Dataset Ingestion):** âœ… COMPLETE
- **Sprint 2 (Segment Extraction):** âœ… COMPLETE
- **Sprint 3 (VAD Baseline):** âœ… COMPLETE - Silero-VAD baseline established
- **Sprint 4 (Model Inference):** ðŸ”œ Ready to start
- **Dataset:** âœ… 1,016 clean segments (640 SPEECH + 376 NONSPEECH)

## ðŸŽ‰ Sprint 3 Complete - VAD Baseline Established

### Silero-VAD Performance
- **SPEECH (AVA + VoxConverse):** 95-100% accuracy
- **NONSPEECH (ESC-50 Clean):** 98-100% accuracy on segments â‰¥100ms
- **Latency:** ~10-20ms (suitable for real-time applications)

### Dataset Composition (1,016 segments total)

**SPEECH Segments (640 total)**
- AVA-Speech: 320 segments (8 durations Ã— 40 each)
- VoxConverse: 320 segments (8 durations Ã— 40 each)

**NONSPEECH Segments (376 total - cleaned)**
- ESC-50: 376 segments (8 durations Ã— 47 each)
- **Ambiguous sounds removed:** Human sounds (breathing, clapping, sneezing, etc.) and animal vocalizations (cat, bird, pig, etc.) eliminated for clear binary classification

**Durations:** 20, 40, 60, 80, 100, 200, 500, 1000 ms

**Clean categories (23 total):**
- Mechanical: airplane, car_horn, chainsaw, engine, helicopter, train, vacuum_cleaner, washing_machine
- Environmental: crackling_fire, rain, sea_waves, thunderstorm, water_drops, wind
- Urban: clock_alarm, clock_tick, glass_breaking, keyboard_typing, siren, toilet_flush
- Actions: can_opening, door_wood_creaks, pouring_water

### Safety Buffer Implementation
- 1-second safety buffers at start/end of all ground truth intervals
- Ensures 100% label confidence (no boundary ambiguity)

## Quick Start

### 1. Environment Setup

```bash
conda activate opro
pip install -e .
```

### 2. Generate Segments

```bash
# AVA-Speech (320 SPEECH segments)
python scripts/make_segments_ava.py --durations 20 40 60 80 100 200 500 1000 --max-per-duration 40

# VoxConverse (320 SPEECH segments)
python scripts/make_segments_voxconverse.py --durations 20 40 60 80 100 200 500 1000 --max-per-duration 40

# ESC-50 Clean (376 NONSPEECH segments - ambiguous sounds removed)
python scripts/make_segments_esc50.py --durations 20 40 60 80 100 200 500 1000 --max-per-duration 80
```

### 3. Run VAD Baseline

```bash
# Evaluate on SPEECH
python scripts/run_vad_baseline.py --segments-dir data/segments/ava_speech/train

# Evaluate on NONSPEECH
python scripts/run_vad_baseline.py --segments-dir data/segments/esc50/nonspeech

# Custom threshold
python scripts/run_vad_baseline.py --segments-dir data/segments/voxconverse/dev --threshold 0.5
```

### 4. Validate Segments

```bash
# Interactive player (WSL-compatible)
python scripts/play_segments_interactive.py --segments-dir data/segments/ava_speech/train --min-duration 500
```

### 5. Verify Dataset

```python
import pandas as pd

ava = pd.read_parquet('data/segments/ava_speech/train/segments_metadata.parquet')
vox = pd.read_parquet('data/segments/voxconverse/dev/segments_metadata.parquet')
esc = pd.read_parquet('data/segments/esc50/nonspeech/segments.parquet')

print(f"SPEECH: {len(ava) + len(vox)}, NONSPEECH: {len(esc)}")
# Output: SPEECH: 640, NONSPEECH: 376

# Check ESC-50 categories (should be 23 clean categories)
print(f"NONSPEECH categories: {esc['condition'].nunique()}")
print(esc['condition'].value_counts().sort_index())
```

## Project Structure

```
qwen-speech-min/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ make_segments_ava.py          # Generate AVA segments
â”‚   â”œâ”€â”€ make_segments_voxconverse.py  # Generate VoxConverse segments
â”‚   â”œâ”€â”€ make_segments_esc50.py        # Generate ESC-50 NONSPEECH (cleaned)
â”‚   â”œâ”€â”€ clean_esc50_dataset.py        # Remove ambiguous sounds
â”‚   â”œâ”€â”€ run_vad_baseline.py           # Evaluate Silero-VAD
â”‚   â””â”€â”€ play_segments_interactive.py  # Validate segments (WSL-compatible)
â”œâ”€â”€ src/qsm/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loaders.py   # RTTM, AVA-Speech loaders
â”‚   â”‚   â””â”€â”€ slicing.py   # Segment extraction with safety buffers
â”‚   â””â”€â”€ vad/
â”‚       â”œâ”€â”€ base.py      # VAD abstract interface
â”‚       â””â”€â”€ silero.py    # Silero-VAD implementation
â”œâ”€â”€ data/segments/
â”‚   â”œâ”€â”€ ava_speech/train/      # 320 AVA SPEECH segments
â”‚   â”œâ”€â”€ voxconverse/dev/       # 320 VoxConverse SPEECH segments
â”‚   â””â”€â”€ esc50/nonspeech/       # 376 ESC-50 NONSPEECH segments (cleaned)
â””â”€â”€ results/vad_baseline/      # Silero-VAD evaluation results
```

## Datasets

### AVA-Speech
- Frame-level annotations (25fps, ~40ms resolution)
- Conditions: clean, music, noise
- Usage: 320 SPEECH segments

### VoxConverse
- RTTM annotations
- Multi-speaker conversations
- Usage: 320 SPEECH segments

### ESC-50 (Cleaned)
- Environmental Sound Classification
- **23 clean categories** (41.2% of original dataset removed)
- **Removed:** Human sounds, animal vocalizations, ambiguous sounds
- 100% guaranteed no speech or speech-like sounds
- Usage: 376 NONSPEECH segments

## VAD Baseline

### Silero-VAD
- **Neural network** based VAD (superior to WebRTC for balanced classification)
- Frame duration: ~32ms (window_size=512 samples at 16kHz)
- Threshold: 0.5 (configurable)
- **Performance:**
  - SPEECH: 95-100% accuracy
  - NONSPEECH: 98-100% accuracy (segments â‰¥100ms)
  - Latency: 10-20ms

### Why Silero over WebRTC?
WebRTC-VAD was designed for telephony with high recall optimization (catch all speech), resulting in 87% false positive rate on clean NONSPEECH. Silero-VAD provides balanced precision and recall suitable for binary classification.

## Safety Buffers

All segments extracted with 1-second safety buffers:

```
Ground truth interval: [10.0s, 15.0s] (5 seconds SPEECH)
Safety buffer exclusions: [10.0, 11.0] and [14.0, 15.0]
Valid extraction zone: [11.0, 14.0] (3 seconds)
```

This guarantees high-confidence labels for psychometric threshold measurement.

## Architecture

### Qwen2-Audio Temporal Resolution
- Mel spectrogram: 25ms window / 10ms hop
- Pooling: Ã—2 stride
- Effective resolution: ~40ms per output frame

### Target Durations
Based on Qwen2-Audio's ~40ms frame resolution:
```
[20, 40, 60, 80, 100, 200, 500, 1000] ms
```

## Dependencies

Core:
- `torch`, `torchaudio` (Silero-VAD)
- `pandas`, `pyannote.core`, `soundfile`
- `sklearn` (metrics)

See [pyproject.toml](pyproject.toml) for complete list.

## Documentation

- [INSTALL.md](INSTALL.md) - Installation guide
- [QUICK_START.md](QUICK_START.md) - Segment generation guide
- [SPRINT_0_COMPLETE.md](SPRINT_0_COMPLETE.md) - Sprint 0 summary

## Results

VAD baseline results available in `results/vad_baseline/`:
- AVA-Speech: [results/vad_baseline/ava_speech/](results/vad_baseline/ava_speech/)
- VoxConverse: [results/vad_baseline/voxconverse/](results/vad_baseline/voxconverse/)
- ESC-50 Clean: [results/vad_baseline/esc50/](results/vad_baseline/esc50/)

## References

- [Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio)
- [AVA-Speech](https://research.google.com/ava/download.html)
- [VoxConverse](https://github.com/joonson/voxconverse)
- [ESC-50](https://github.com/karolpiczak/ESC-50)
- [Silero-VAD](https://github.com/snakers4/silero-vad)

## License

Apache-2.0

---

**Sprint 3 Complete:** Silero-VAD baseline established with 95-100% accuracy on clean dataset. Ready for Sprint 4 (Qwen2-Audio Model Inference).
