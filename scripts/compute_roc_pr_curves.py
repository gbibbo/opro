#!/usr/bin/env python3
"""
ROC and PR Curve Analysis with Bootstrap Confidence Intervals

Computes ROC-AUC and PR-AUC with bootstrap confidence intervals from
model predictions. Works with CSV files generated by evaluate_with_logits.py.

Usage:
    python scripts/compute_roc_pr_curves.py \
        --predictions results/ablations/LORA_attn_mlp_seed42.csv \
        --output_dir results/roc_pr_analysis/LORA_attn_mlp_seed42 \
        --n_bootstrap 10000

Generates:
    - ROC curve plot with bootstrap CI shading
    - PR curve plot with bootstrap CI shading
    - JSON file with AUC values and CIs
    - CSV with threshold analysis
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import roc_curve, precision_recall_curve, auc, roc_auc_score, average_precision_score
from scipy import stats


def bootstrap_auc(y_true, y_score, metric='roc', n_bootstrap=10000, confidence_level=0.95):
    """
    Compute AUC with bootstrap confidence interval.

    Args:
        y_true: True binary labels (0 or 1)
        y_score: Predicted scores/probabilities
        metric: 'roc' or 'pr' for ROC-AUC or PR-AUC
        n_bootstrap: Number of bootstrap samples
        confidence_level: Confidence level for CI (default 0.95)

    Returns:
        dict with auc, ci_lower, ci_upper, bootstrap_samples
    """
    n = len(y_true)
    bootstrap_aucs = []

    np.random.seed(42)  # For reproducibility

    for _ in range(n_bootstrap):
        # Sample with replacement
        indices = np.random.choice(n, size=n, replace=True)
        y_true_boot = y_true[indices]
        y_score_boot = y_score[indices]

        # Check if both classes present
        if len(np.unique(y_true_boot)) < 2:
            continue  # Skip if only one class

        # Compute AUC
        if metric == 'roc':
            try:
                auc_boot = roc_auc_score(y_true_boot, y_score_boot)
                bootstrap_aucs.append(auc_boot)
            except:
                continue
        elif metric == 'pr':
            try:
                auc_boot = average_precision_score(y_true_boot, y_score_boot)
                bootstrap_aucs.append(auc_boot)
            except:
                continue

    bootstrap_aucs = np.array(bootstrap_aucs)

    # Compute confidence interval
    alpha = 1 - confidence_level
    ci_lower = np.percentile(bootstrap_aucs, alpha/2 * 100)
    ci_upper = np.percentile(bootstrap_aucs, (1 - alpha/2) * 100)

    # Original AUC
    if metric == 'roc':
        original_auc = roc_auc_score(y_true, y_score)
    elif metric == 'pr':
        original_auc = average_precision_score(y_true, y_score)

    return {
        'auc': original_auc,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'bootstrap_samples': bootstrap_aucs,
        'n_bootstrap': len(bootstrap_aucs)
    }


def plot_roc_curve(y_true, y_score, output_path, title="ROC Curve", bootstrap_ci=None):
    """Plot ROC curve with optional bootstrap confidence interval shading."""
    fig, ax = plt.subplots(figsize=(8, 8))

    # Compute ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    ax.plot(fpr, tpr, color='darkorange', lw=2,
           label=f'ROC curve (AUC = {roc_auc:.3f})')

    # Add bootstrap CI if provided
    if bootstrap_ci is not None:
        ax.fill_between([0, 1], [bootstrap_ci['ci_lower'], bootstrap_ci['ci_lower']],
                       [bootstrap_ci['ci_upper'], bootstrap_ci['ci_upper']],
                       alpha=0.2, color='darkorange',
                       label=f'95% CI [{bootstrap_ci["ci_lower"]:.3f}, {bootstrap_ci["ci_upper"]:.3f}]')

    # Plot diagonal reference line
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')

    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate', fontsize=12)
    ax.set_ylabel('True Positive Rate', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.legend(loc="lower right", fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"ROC curve saved to: {output_path}")


def plot_pr_curve(y_true, y_score, output_path, title="Precision-Recall Curve", bootstrap_ci=None):
    """Plot PR curve with optional bootstrap confidence interval shading."""
    fig, ax = plt.subplots(figsize=(8, 8))

    # Compute PR curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_score)
    pr_auc = auc(recall, precision)
    ap = average_precision_score(y_true, y_score)

    # Plot PR curve
    ax.plot(recall, precision, color='blue', lw=2,
           label=f'PR curve (AP = {ap:.3f}, AUC = {pr_auc:.3f})')

    # Add bootstrap CI if provided
    if bootstrap_ci is not None:
        ax.fill_between([0, 1], [bootstrap_ci['ci_lower'], bootstrap_ci['ci_lower']],
                       [bootstrap_ci['ci_upper'], bootstrap_ci['ci_upper']],
                       alpha=0.2, color='blue',
                       label=f'95% CI [{bootstrap_ci["ci_lower"]:.3f}, {bootstrap_ci["ci_upper"]:.3f}]')

    # Plot baseline (prevalence)
    prevalence = y_true.sum() / len(y_true)
    ax.plot([0, 1], [prevalence, prevalence], color='navy', lw=2, linestyle='--',
           label=f'Random classifier (prevalence={prevalence:.3f})')

    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Recall', fontsize=12)
    ax.set_ylabel('Precision', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.legend(loc="lower left", fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"PR curve saved to: {output_path}")


def analyze_thresholds(y_true, y_score, output_csv):
    """Analyze performance at different thresholds."""
    # Compute metrics at different thresholds
    thresholds = np.linspace(0, 1, 101)
    results = []

    for threshold in thresholds:
        y_pred = (y_score >= threshold).astype(int)

        # Confusion matrix elements
        tp = ((y_pred == 1) & (y_true == 1)).sum()
        tn = ((y_pred == 0) & (y_true == 0)).sum()
        fp = ((y_pred == 1) & (y_true == 0)).sum()
        fn = ((y_pred == 0) & (y_true == 1)).sum()

        # Metrics
        accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

        results.append({
            'threshold': threshold,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'tp': int(tp),
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn)
        })

    df = pd.DataFrame(results)
    df.to_csv(output_csv, index=False)
    print(f"Threshold analysis saved to: {output_csv}")

    return df


def main():
    parser = argparse.ArgumentParser(description='Compute ROC and PR curves with bootstrap CI')
    parser.add_argument('--predictions', type=str, required=True,
                       help='Path to predictions CSV (must have logit_diff and ground_truth columns)')
    parser.add_argument('--output_dir', type=str, default='results/roc_pr_analysis',
                       help='Output directory for results')
    parser.add_argument('--n_bootstrap', type=int, default=10000,
                       help='Number of bootstrap samples for CI (default: 10000)')

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load predictions
    print(f"Loading predictions from: {args.predictions}")
    df = pd.read_csv(args.predictions)

    # Check required columns
    if 'logit_diff' not in df.columns or 'ground_truth' not in df.columns:
        raise ValueError("CSV must contain 'logit_diff' and 'ground_truth' columns")

    # Convert ground truth to binary (1 = SPEECH, 0 = NONSPEECH)
    y_true = (df['ground_truth'] == 'SPEECH').astype(int).values

    # Use logit_diff as score (higher = more likely SPEECH)
    y_score = df['logit_diff'].values

    # Convert logit_diff to probability using sigmoid
    # p(SPEECH) = sigmoid(logit_A - logit_B)
    y_prob = 1 / (1 + np.exp(-y_score))

    print(f"\nDataset: {len(df)} samples")
    print(f"  SPEECH (positive class): {y_true.sum()} samples")
    print(f"  NONSPEECH (negative class): {(1-y_true).sum()} samples")

    # Compute ROC-AUC with bootstrap CI
    print(f"\nComputing ROC-AUC with {args.n_bootstrap} bootstrap samples...")
    roc_results = bootstrap_auc(y_true, y_prob, metric='roc', n_bootstrap=args.n_bootstrap)

    print(f"\nROC-AUC Results:")
    print(f"  AUC: {roc_results['auc']:.4f}")
    print(f"  95% CI: [{roc_results['ci_lower']:.4f}, {roc_results['ci_upper']:.4f}]")
    print(f"  Bootstrap samples used: {roc_results['n_bootstrap']}")

    # Compute PR-AUC with bootstrap CI
    print(f"\nComputing PR-AUC with {args.n_bootstrap} bootstrap samples...")
    pr_results = bootstrap_auc(y_true, y_prob, metric='pr', n_bootstrap=args.n_bootstrap)

    print(f"\nPR-AUC Results:")
    print(f"  Average Precision: {pr_results['auc']:.4f}")
    print(f"  95% CI: [{pr_results['ci_lower']:.4f}, {pr_results['ci_upper']:.4f}]")
    print(f"  Bootstrap samples used: {pr_results['n_bootstrap']}")

    # Plot ROC curve
    print("\nGenerating ROC curve...")
    plot_roc_curve(y_true, y_prob, output_dir / 'roc_curve.png',
                  title="ROC Curve", bootstrap_ci=roc_results)

    # Plot PR curve
    print("Generating PR curve...")
    plot_pr_curve(y_true, y_prob, output_dir / 'pr_curve.png',
                 title="Precision-Recall Curve", bootstrap_ci=pr_results)

    # Threshold analysis
    print("\nAnalyzing thresholds...")
    threshold_df = analyze_thresholds(y_true, y_prob, output_dir / 'threshold_analysis.csv')

    # Find optimal threshold (max F1)
    optimal_idx = threshold_df['f1_score'].idxmax()
    optimal_row = threshold_df.iloc[optimal_idx]

    print(f"\nOptimal Threshold (max F1):")
    print(f"  Threshold: {optimal_row['threshold']:.3f}")
    print(f"  F1-score: {optimal_row['f1_score']:.4f}")
    print(f"  Accuracy: {optimal_row['accuracy']:.4f}")
    print(f"  Precision: {optimal_row['precision']:.4f}")
    print(f"  Recall: {optimal_row['recall']:.4f}")

    # Save results summary
    summary = {
        'roc_auc': roc_results['auc'],
        'roc_auc_ci_lower': roc_results['ci_lower'],
        'roc_auc_ci_upper': roc_results['ci_upper'],
        'pr_auc': pr_results['auc'],
        'pr_auc_ci_lower': pr_results['ci_lower'],
        'pr_auc_ci_upper': pr_results['ci_upper'],
        'optimal_threshold': optimal_row['threshold'],
        'optimal_f1': optimal_row['f1_score'],
        'optimal_accuracy': optimal_row['accuracy'],
        'n_samples': len(df),
        'n_positive': int(y_true.sum()),
        'n_negative': int((1-y_true).sum()),
        'n_bootstrap': args.n_bootstrap
    }

    import json
    with open(output_dir / 'auc_results.json', 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"\nResults summary saved to: {output_dir / 'auc_results.json'}")
    print(f"\nAll outputs saved to: {output_dir}")


if __name__ == "__main__":
    main()
