#!/usr/bin/env python3
"""
Generate Pipeline Execution Report

Consolidates all results into a comprehensive markdown report.

Usage:
    python scripts/generate_pipeline_report.py \
        --config config/pipeline_config.yaml \
        --results_dir results/ \
        --output_report PIPELINE_EXECUTION_REPORT.md
"""

import argparse
from pathlib import Path
import yaml
import pandas as pd
from datetime import datetime
import subprocess


def get_git_info():
    """Get current git commit and status"""
    try:
        commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()
        branch = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()
        return commit[:8], branch
    except:
        return "unknown", "unknown"


def load_metrics(results_dir):
    """Load key metrics from results"""
    metrics = {}

    # Aggregated results
    agg_file = results_dir / 'aggregated' / 'aggregate_metrics.csv'
    if agg_file.exists():
        df = pd.read_csv(agg_file)
        metrics['aggregated'] = df.to_dict('records')

    # Comparison table
    comp_file = results_dir / 'comparison' / 'comparison_table.csv'
    if comp_file.exists():
        df = pd.read_csv(comp_file)
        metrics['comparison'] = df.to_dict('records')

    return metrics


def generate_report(config, results_dir, output_file):
    """Generate comprehensive markdown report"""
    commit, branch = get_git_info()

    report = f"""# PIPELINE EXECUTION REPORT

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Git Commit:** `{commit}` (branch: `{branch}`)
**Pipeline Version:** 1.0.0

---

## Executive Summary

This report documents the complete execution of the speech detection pipeline
using Qwen2-Audio-7B fine-tuned with LoRA/QLoRA.

### Key Results

"""

    # Load metrics
    metrics = load_metrics(results_dir)

    if 'aggregated' in metrics:
        report += "\n**Multi-Seed Aggregated Results:**\n\n"
        for m in metrics['aggregated']:
            report += f"- Seed {m.get('seed', 'N/A')}: {m.get('accuracy', 0):.1f}% accuracy\n"

    report += f"""

---

## Configuration

### Hardware
- Minimum RAM: {config.get('environment', {}).get('min_ram_gb', 'N/A')} GB
- Minimum VRAM: {config.get('environment', {}).get('min_vram_gb', 'N/A')} GB

### Datasets
"""

    for ds_name, ds_config in config.get('datasets', {}).items():
        if isinstance(ds_config, dict) and ds_config.get('enabled', True):
            report += f"- **{ds_name.upper()}**: {ds_config.get('description', 'N/A')}\n"

    report += f"""

### Training Configuration
- Seeds: {config.get('training', {}).get('seeds', [])}
- Base Model: `{config.get('training', {}).get('base_model', 'N/A')}`
- LoRA r: {config.get('training', {}).get('lora', {}).get('r', 'N/A')}
- Epochs: {config.get('training', {}).get('hyperparameters', {}).get('num_epochs', 'N/A')}

### Experimental Design
- Durations: {config.get('experimental_design', {}).get('durations_ms', [])} ms
- SNR Levels: {config.get('experimental_design', {}).get('snr_levels_db', [])} dB

---

## Results

### Model Comparison

"""

    if 'comparison' in metrics:
        report += "| Method | Accuracy | SPEECH | NONSPEECH | ROC-AUC |\n"
        report += "|--------|----------|--------|-----------|----------|\n"
        for m in metrics['comparison']:
            report += f"| {m.get('method', 'N/A')} | {m.get('overall', 0):.1f}% | {m.get('speech', 0):.1f}% | {m.get('nonspeech', 0):.1f}% | {m.get('roc_auc', 'N/A')} |\n"

    report += """

---

## Artifacts

### Checkpoints
"""
    checkpoints_dir = Path('checkpoints')
    if checkpoints_dir.exists():
        for cp in sorted(checkpoints_dir.glob('qwen_lora_seed*')):
            report += f"- `{cp.name}/final/`\n"

    report += """

### Result Files
"""
    for subdir in ['dev_eval', 'test_final', 'psychometric_curves', 'comparison']:
        result_subdir = results_dir / subdir
        if result_subdir.exists():
            report += f"\n**{subdir.replace('_', ' ').title()}:**\n"
            for f in sorted(result_subdir.glob('*.*'))[:5]:
                report += f"- `{f.relative_to(results_dir.parent)}`\n"

    report += f"""

---

## Reproducibility

All results can be reproduced by running:

```bash
python run_complete_pipeline.py --config config/pipeline_config.yaml
```

**Environment:**
- PyTorch: (check `pip show torch`)
- Transformers: (check `pip show transformers`)
- PEFT: (check `pip show peft`)

---

*Report generated by `scripts/generate_pipeline_report.py`*
"""

    # Write report
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ“ Report saved to: {output_file}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=Path, required=True)
    parser.add_argument('--results_dir', type=Path, required=True)
    parser.add_argument('--output_report', type=Path, default='PIPELINE_EXECUTION_REPORT.md')
    args = parser.parse_args()

    # Load config
    with open(args.config) as f:
        config = yaml.safe_load(f)

    generate_report(config, args.results_dir, args.output_report)


if __name__ == "__main__":
    main()
