#!/usr/bin/env python3
"""
Evaluate Qwen2-Audio on psychoacoustic condition variants.

Processes all 6,264 variants (SNR sweep + band-limiting) generated by
build_conditions.py and saves results to Parquet for analysis.

Input:
    - data/processed/conditions/conditions_manifest.parquet

Output:
    - results/qwen_conditions.parquet (detailed predictions)
    - results/qwen_conditions_summary.parquet (aggregated stats)
"""

import sys
from pathlib import Path
import pandas as pd
import time
from tqdm import tqdm
import argparse

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
from qsm.models import Qwen2AudioClassifier


def main():
    parser = argparse.ArgumentParser(description="Evaluate Qwen2-Audio on psychoacoustic conditions")
    parser.add_argument(
        "--manifest",
        type=str,
        default="data/processed/conditions/conditions_manifest.parquet",
        help="Path to conditions manifest (Parquet)",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="results",
        help="Directory to save results",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=100,
        help="Batch size for progress updates",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Device to run model on (cuda/cpu)",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Limit to first N samples (for testing)",
    )
    args = parser.parse_args()

    print("=" * 80)
    print("QWEN2-AUDIO EVALUATION: Psychoacoustic Conditions")
    print("=" * 80)

    # Load manifest
    print(f"\nLoading manifest: {args.manifest}")
    manifest_path = Path(args.manifest)
    if not manifest_path.exists():
        print(f"ERROR: Manifest not found at {manifest_path}")
        print("\nPlease run: python scripts/build_conditions.py")
        sys.exit(1)

    df = pd.read_parquet(manifest_path)

    if args.max_samples:
        print(f"Limiting to first {args.max_samples} samples for testing")
        df = df.head(args.max_samples)

    print(f"Total variants to evaluate: {len(df):,}")
    print(f"\nBreakdown by variant type:")
    print(df["variant_type"].value_counts().to_string())
    print(f"\nBreakdown by label:")
    print(df["label"].value_counts().to_string())

    # Load model
    print(f"\nLoading Qwen2-Audio model (device={args.device})...")
    model = Qwen2AudioClassifier(
        device=args.device,
        torch_dtype="float16",
        load_in_4bit=True,
        auto_pad=False,  # Audio is already padded to 2000ms
    )

    print("\nPrompt configuration:")
    print(f"  System: {model.system_prompt}")
    print(f"  User: {model.user_prompt[:80]}...")

    # Initialize results tracking
    results = []
    start_time = time.time()

    # Process all samples
    print(f"\nStarting evaluation...")
    print("=" * 80)

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Evaluating", unit="sample"):
        audio_path = Path(row["audio_path"])

        # Run prediction
        try:
            pred = model.predict(audio_path)

            # Determine correctness (normalize NONSPEECH variants)
            ground_truth = row["label"]
            pred_normalized = pred.label.replace("-", "").replace("_", "")
            gt_normalized = ground_truth.replace("-", "").replace("_", "")
            is_correct = (pred_normalized == gt_normalized)

            results.append({
                # Identification
                "clip_id": row["clip_id"],
                "original_path": row["original_path"],
                "audio_path": str(audio_path),

                # Ground truth metadata
                "ground_truth": ground_truth,
                "duration_ms": row["duration_ms"],

                # Manipulation parameters
                "variant_type": row["variant_type"],
                "snr_db": row.get("snr_db", None),
                "band_filter": row.get("band_filter", None),
                "rir_id": row.get("rir_id", None),
                "T60": row.get("T60", None),

                # Model output
                "predicted": pred.label,
                "confidence": pred.confidence,
                "correct": is_correct,
                "raw_output": pred.raw_output,
                "latency_ms": pred.latency_ms,
            })

        except Exception as e:
            print(f"\nERROR processing {audio_path}: {e}")
            results.append({
                "clip_id": row["clip_id"],
                "original_path": row["original_path"],
                "audio_path": str(audio_path),
                "ground_truth": row["label"],
                "duration_ms": row["duration_ms"],
                "variant_type": row["variant_type"],
                "snr_db": row.get("snr_db", None),
                "band_filter": row.get("band_filter", None),
                "rir_id": row.get("rir_id", None),
                "T60": row.get("T60", None),
                "predicted": None,
                "confidence": None,
                "correct": False,
                "raw_output": str(e),
                "latency_ms": None,
            })

    # Calculate overall statistics
    total_time = time.time() - start_time
    results_df = pd.DataFrame(results)

    # Overall metrics
    overall_accuracy = results_df["correct"].mean() * 100
    total_samples = len(results_df)
    total_correct = results_df["correct"].sum()
    avg_latency = results_df["latency_ms"].mean()
    avg_time_per_sample = total_time / total_samples

    # Print summary
    print(f"\n{'=' * 80}")
    print("EVALUATION COMPLETE")
    print(f"{'=' * 80}\n")

    print(f"Overall Statistics:")
    print(f"  Total samples: {total_samples:,}")
    print(f"  Correct: {total_correct:,}")
    print(f"  Accuracy: {overall_accuracy:.2f}%")
    print(f"  Avg latency: {avg_latency:.1f}ms")
    print(f"  Total time: {total_time:.1f}s ({total_time / 60:.1f} minutes)")
    print(f"  Time per sample: {avg_time_per_sample:.2f}s")
    print(f"  Samples per minute: {60 / avg_time_per_sample:.1f}")

    # Breakdown by variant type
    print(f"\nAccuracy by Variant Type:")
    print("-" * 50)
    for variant_type in results_df["variant_type"].unique():
        subset = results_df[results_df["variant_type"] == variant_type]
        acc = subset["correct"].mean() * 100
        count = len(subset)
        print(f"  {variant_type:12s}: {acc:6.2f}% (n={count:,})")

    # Breakdown by SNR (if applicable)
    if results_df["snr_db"].notna().any():
        print(f"\nAccuracy by SNR Level:")
        print("-" * 50)
        snr_summary = results_df[results_df["snr_db"].notna()].groupby("snr_db")["correct"].agg(
            accuracy=lambda x: x.mean() * 100,
            count="count"
        ).sort_index()
        for snr, row in snr_summary.iterrows():
            print(f"  {snr:+4.0f} dB: {row['accuracy']:6.2f}% (n={int(row['count']):,})")

    # Breakdown by band filter (if applicable)
    if results_df["band_filter"].notna().any():
        print(f"\nAccuracy by Band Filter:")
        print("-" * 50)
        band_summary = results_df[results_df["band_filter"].notna()].groupby("band_filter")["correct"].agg(
            accuracy=lambda x: x.mean() * 100,
            count="count"
        )
        for band, row in band_summary.iterrows():
            print(f"  {band:12s}: {row['accuracy']:6.2f}% (n={int(row['count']):,})")

    # Breakdown by duration
    print(f"\nAccuracy by Duration:")
    print("-" * 50)
    duration_summary = results_df.groupby("duration_ms")["correct"].agg(
        accuracy=lambda x: x.mean() * 100,
        count="count"
    ).sort_index()
    for dur, row in duration_summary.iterrows():
        print(f"  {int(dur):4d} ms: {row['accuracy']:6.2f}% (n={int(row['count']):,})")

    # Breakdown by ground truth
    print(f"\nAccuracy by Ground Truth:")
    print("-" * 50)
    for gt in results_df["ground_truth"].unique():
        subset = results_df[results_df["ground_truth"] == gt]
        acc = subset["correct"].mean() * 100
        count = len(subset)
        print(f"  {gt:12s}: {acc:6.2f}% (n={count:,})")

    # Save detailed results
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    detailed_path = output_dir / "qwen_conditions.parquet"
    results_df.to_parquet(detailed_path, index=False)
    print(f"\nDetailed results saved to: {detailed_path}")

    # Create summary statistics (aggregated by manipulation parameters)
    summary_groups = []

    # SNR summary
    if results_df["snr_db"].notna().any():
        snr_summary = results_df[results_df["snr_db"].notna()].groupby(
            ["variant_type", "snr_db", "duration_ms", "ground_truth"]
        )["correct"].agg(
            accuracy=lambda x: x.mean() * 100,
            n_correct="sum",
            n_total="count"
        ).reset_index()
        summary_groups.append(snr_summary)

    # Band filter summary
    if results_df["band_filter"].notna().any():
        band_summary = results_df[results_df["band_filter"].notna()].groupby(
            ["variant_type", "band_filter", "duration_ms", "ground_truth"]
        )["correct"].agg(
            accuracy=lambda x: x.mean() * 100,
            n_correct="sum",
            n_total="count"
        ).reset_index()
        summary_groups.append(band_summary)

    # Save summary
    if summary_groups:
        summary_df = pd.concat(summary_groups, ignore_index=True)
        summary_path = output_dir / "qwen_conditions_summary.parquet"
        summary_df.to_parquet(summary_path, index=False)
        print(f"Summary statistics saved to: {summary_path}")

    print(f"\n{'=' * 80}")
    print("Done!")
    print(f"{'=' * 80}")


if __name__ == "__main__":
    main()
